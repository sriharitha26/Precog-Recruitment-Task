{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":96561,"status":"ok","timestamp":1721408175315,"user":{"displayName":"Sriharitha","userId":"12204374650022354334"},"user_tz":-330},"id":"2pbqoRGeJxyL","outputId":"7d26fa9e-09e6-483d-ad06-159301f8160b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (9.4.0)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.1+cu121)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.18.1+cu121)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.3)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n","Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python-headless) (1.25.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.15.4)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.0)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n","  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n","  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n","  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n","  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n","  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n","  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n","  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n","  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n","  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","Collecting nvidia-nccl-cu12==2.20.5 (from torch)\n","  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n","Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n","  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n","Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.1)\n","Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n","  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.53.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.1)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n","Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n","Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105\n"]}],"source":["!pip install opencv-python-headless pillow torch torchvision pandas matplotlib"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":33073,"status":"ok","timestamp":1721408300997,"user":{"displayName":"Sriharitha","userId":"12204374650022354334"},"user_tz":-330},"id":"nX_901P0SsDG","outputId":"cda6e763-3371-4e4f-9a2a-d3943d841636"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"dhCQQcNV7kPZ","outputId":"c2ce2479-387c-4489-92e6-e5b61d34de2f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Processing 87420.png...\n","Processing 47162.png...\n","Processing 02358.png...\n","Processing 85237.png...\n","Processing 76432.png...\n","Processing 97814.png...\n","Processing 47189.png...\n","Processing 81904.png...\n","Processing 86925.png...\n","Processing 78351.png...\n","Processing 90362.png...\n","Processing 87391.png...\n","Processing 96713.png...\n","Processing 78345.png...\n","Processing 81092.png...\n","Processing 46730.png...\n","Processing 78190.png...\n","Processing 80165.png...\n","Processing 92561.png...\n","Processing 02614.png...\n","Processing 84317.png...\n","Processing 86514.png...\n","Processing 81509.png...\n","Processing 82014.png...\n","Processing 98702.png...\n","Processing 98064.png...\n","Processing 93125.png...\n","Processing 98716.png...\n","Processing 76803.png...\n","Processing 94138.png...\n","Processing 82941.png...\n","Processing 85960.png...\n","Processing 91083.png...\n","Processing 85790.png...\n","Processing 90564.png...\n","Processing 01269.png...\n","Processing 89035.png...\n","Processing 83256.png...\n","Processing 02984.png...\n","Processing 87024.png...\n","Processing 01527.png...\n","Processing 93051.png...\n","Processing 93045.png...\n","Processing 97586.png...\n","Processing 96840.png...\n","Processing 79305.png...\n","Processing 79463.png...\n","Processing 81496.png...\n","Processing 85196.png...\n","Processing 83295.png...\n","Processing 93086.png...\n","Processing 96471.png...\n","Processing 86104.png...\n","Processing 84061.png...\n","Processing 02576.png...\n","Processing 96317.png...\n","Processing 92603.png...\n","Processing 92617.png...\n","Processing 02947.png...\n","Processing 83726.png...\n","Processing 47016.png...\n","Processing 94528.png...\n","Processing 79846.png...\n","Processing 93521.png...\n","Processing 97021.png...\n","Processing 93247.png...\n","Processing 78594.png...\n","Processing 83901.png...\n","Processing 97035.png...\n","Processing 78219.png...\n","Processing 93284.png...\n","Processing 85394.png...\n","Processing 78231.png...\n","Processing 83095.png...\n","Processing 78541.png...\n","Processing 91487.png...\n","Processing 93286.png...\n","Processing 95813.png...\n","Processing 79105.png...\n","Processing 80239.png...\n","Processing 02548.png...\n","Processing 98310.png...\n","Processing 87230.png...\n","Processing 83056.png...\n","Processing 97023.png...\n","Processing 91320.png...\n","Processing 03124.png...\n","Processing 93251.png...\n","Processing 93245.png...\n","Processing 79850.png...\n","Processing 95608.png...\n","Processing 92601.png...\n","Processing 91308.png...\n","Processing 92167.png...\n","Processing 82406.png...\n","Processing 02951.png...\n","Processing 93084.png...\n","Processing 96315.png...\n","Processing 85369.png...\n","Processing 82360.png...\n","Processing 96301.png...\n","Processing 02945.png...\n","Processing 91863.png...\n","Processing 95387.png...\n","Processing 79461.png...\n","Processing 78031.png...\n","Processing 02789.png...\n","Processing 01243.png...\n","Processing 96103.png...\n","Processing 81457.png...\n","Processing 90572.png...\n","Processing 92365.png...\n","Processing 78964.png...\n","Processing 89745.png...\n","Processing 90214.png...\n","Processing 82604.png...\n","Processing 92417.png...\n","Processing 82943.png...\n","Processing 02831.png...\n","Processing 78623.png...\n","Processing 01492.png...\n","Processing 85976.png...\n","Processing 02158.png...\n","Processing 87146.png...\n","Processing 80629.png...\n","Processing 91056.png...\n","Processing 81245.png...\n","Processing 84329.png...\n","Processing 98072.png...\n","Processing 91042.png...\n","Processing 78192.png...\n","Processing 91724.png...\n","Processing 98714.png...\n","Processing 87634.png...\n","Processing 85023.png...\n","Processing 81523.png...\n","Processing 87620.png...\n","Processing 02164.png...\n","Processing 46732.png...\n","Processing 86502.png...\n","Processing 81279.png...\n","Processing 76815.png...\n","Processing 91730.png...\n","Processing 80615.png...\n","Processing 93127.png...\n","Processing 94106.png...\n","Processing 80173.png...\n","Processing 82016.png...\n","Processing 97180.png...\n","Processing 91283.png...\n","Processing 86927.png...\n","Processing 47809.png...\n","Processing 95783.png...\n","Processing 01874.png...\n","Processing 91526.png...\n","Processing 97816.png...\n","Processing 90638.png...\n","Processing 78409.png...\n","Processing 91240.png...\n","Processing 97625.png...\n","Processing 87436.png...\n","Processing 98270.png...\n","Processing 47612.png...\n","Processing 97143.png...\n","Processing 81047.png...\n","Processing 01653.png...\n","Processing 98516.png...\n","Processing 83650.png...\n","Processing 85209.png...\n","Processing 90162.png...\n","Processing 92013.png...\n","Processing 96513.png...\n","Processing 86072.png...\n","Processing 84102.png...\n","Processing 84103.png...\n","Processing 86714.png...\n","Processing 94305.png...\n","Processing 91268.png...\n","Processing 96507.png...\n","Processing 80365.png...\n","Processing 89432.png...\n","Processing 87351.png...\n","Processing 86701.png...\n","Processing 90163.png...\n","Processing 84670.png...\n","Processing 02367.png...\n","Processing 97142.png...\n","Processing 81720.png...\n","Processing 83651.png...\n","Processing 98271.png...\n","Processing 87345.png...\n","Processing 97624.png...\n","Processing 98517.png...\n","Processing 95027.png...\n","Processing 81734.png...\n","Processing 91527.png...\n","Processing 01875.png...\n","Processing 90836.png...\n","Processing 87392.png...\n","Processing 78420.png...\n","Processing 83692.png...\n","Processing 93481.png...\n","Processing 96704.png...\n","Processing 89624.png...\n","Processing 86271.png...\n","Processing 89142.png...\n","Processing 89156.png...\n","Processing 90413.png...\n","Processing 94675.png...\n","Processing 80614.png...\n","Processing 80172.png...\n","Processing 82765.png...\n","Processing 02165.png...\n","Processing 96710.png...\n","Processing 97426.png...\n","Processing 94107.png...\n","Processing 93126.png...\n","Processing 76814.png...\n","Processing 86259.png...\n","Processing 98067.png...\n","Processing 81536.png...\n","Processing 93867.png...\n","Processing 98701.png...\n","Processing 46928.png...\n","Processing 87621.png...\n","Processing 81250.png...\n","Processing 83490.png...\n","Processing 81293.png...\n","Processing 89750.png...\n","Processing 85793.png...\n","Processing 94067.png...\n","Processing 87190.png...\n","Processing 01487.png...\n","Processing 82163.png...\n","Processing 02987.png...\n","Processing 47203.png...\n","Processing 01256.png...\n","Processing 02763.png...\n","Processing 90573.png...\n","Processing 83527.png...\n","Processing 78965.png...\n","Processing 97208.png...\n","Processing 92364.png...\n","Processing 92370.png...\n","Processing 01524.png...\n","Processing 97546.png...\n","Processing 92358.png...\n","Processing 93046.png...\n","Processing 85630.png...\n","Processing 91862.png...\n","Processing 95386.png...\n","Processing 79312.png...\n","Processing 80576.png...\n","Processing 96857.png...\n","Processing 80947.png...\n","Processing 87219.png...\n","Processing 02561.png...\n","Processing 93085.png...\n","Processing 83296.png...\n","Processing 01295.png...\n","Processing 84076.png...\n","Processing 47029.png...\n","Processing 92614.png...\n","Processing 96472.png...\n","Processing 82413.png...\n","Processing 83719.png...\n","Processing 94517.png...\n","Processing 96328.png...\n","Processing 98463.png...\n","Processing 47015.png...\n","Processing 03125.png...\n","Processing 79845.png...\n","Processing 87543.png...\n","Processing 98305.png...\n","Processing 01726.png...\n","Processing 76592.png...\n","Processing 95812.png...\n","Processing 91453.png...\n","Processing 95806.png...\n","Processing 79104.png...\n","Processing 83916.png...\n","Processing 85426.png...\n","Processing 87594.png...\n","Processing 79138.png...\n","Processing 95184.png...\n","Processing 93287.png...\n","Processing 83965.png...\n","Processing 01972.png...\n","Processing 93586.png...\n","Processing 95861.png...\n","Processing 79605.png...\n","Processing 91385.png...\n","Processing 92854.png...\n","Processing 95108.png...\n","Processing 01796.png...\n","Processing 85496.png...\n","Processing 89521.png...\n","Processing 94216.png...\n","Processing 97045.png...\n","Processing 83756.png...\n","Processing 80539.png...\n","Processing 85327.png...\n","Processing 96415.png...\n","Processing 82460.png...\n","Processing 87530.png...\n","Processing 79413.png...\n","Processing 97051.png...\n","Processing 98362.png...\n","Processing 83024.png...\n","Processing 78296.png...\n","Processing 89723.png...\n","Processing 91805.png...\n","Processing 96824.png...\n","Processing 01594.png...\n","Processing 95487.png...\n","Processing 46807.png...\n","Processing 02937.png...\n","Processing 78902.png...\n","Processing 46813.png...\n","Processing 81394.png...\n","Processing 93784.png...\n","Processing 47516.png...\n","Processing 92317.png...\n","Processing 82104.png...\n","Processing 76913.png...\n","Processing 87054.png...\n","Processing 80713.png...\n","Processing 95478.png...\n","Processing 97521.png...\n","Processing 78094.png...\n","Processing 46783.png...\n","Processing 85092.png...\n","Processing 87691.png...\n","Processing 02857.png...\n","Processing 95281.png...\n","Processing 01392.png...\n","Processing 82716.png...\n","Processing 01379.png...\n","Processing 46973.png...\n","Processing 89657.png...\n","Processing 01423.png...\n","Processing 91024.png...\n","Processing 02894.png...\n","Processing 81579.png...\n","Processing 91756.png...\n","Processing 87120.png...\n","Processing 85290.png...\n","Processing 90845.png...\n","Processing 78321.png...\n","Processing 92075.png...\n","Processing 96213.png...\n","Processing 76495.png...\n","Processing 80317.png...\n","Processing 97125.png...\n","Processing 91540.png...\n","Processing 95068.png...\n","Processing 81035.png...\n","Processing 98564.png...\n","Processing 90138.png...\n","Processing 95726.png...\n","Processing 83150.png...\n","Processing 98571.png...\n","Processing 98217.png...\n","Processing 97642.png...\n","Processing 81746.png...\n","Processing 93425.png...\n","Processing 87451.png...\n","Processing 97643.png...\n","Processing 02315.png...\n","Processing 81752.png...\n","Processing 98203.png...\n","Processing 02467.png...\n","Processing 89326.png...\n","Processing 01634.png...\n","Processing 83145.png...\n","Processing 86954.png...\n","Processing 92074.png...\n","Processing 80316.png...\n","Processing 82501.png...\n","Processing 85291.png...\n","Processing 78452.png...\n","Processing 81975.png...\n","Processing 01436.png...\n","Processing 78693.png...\n","Processing 98015.png...\n","Processing 83421.png...\n","Processing 89642.png...\n","Processing 47305.png...\n","Processing 94175.png...\n","Processing 93168.png...\n","Processing 80672.png...\n"]}],"source":["import os\n","import torch\n","import torchvision\n","from torchvision import transforms as T\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","import cv2\n","\n","# Faster R-CNN model\n","def get_model():\n","\n","    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n","    model.eval()\n","    return model\n","\n","# Function to transform input image\n","def get_transform():\n","    transform = T.Compose([\n","        T.ToTensor(),  # Convert PIL image or numpy array to a tensor\n","    ])\n","    return transform\n","\n","\n","# Function to perform inference on a single image\n","def detect_objects(image_path, model, transform):\n","\n","    image = Image.open(image_path).convert(\"RGB\")\n","\n","    # Apply the transformation to the image\n","    image_tensor = transform(image).unsqueeze(0)  # Add batch dimension\n","\n","    # Perform the inference\n","    with torch.no_grad():\n","        outputs = model(image_tensor)\n","\n","    return outputs\n","\n","\n","def visualize_results(image_path, outputs, threshold=0.5):\n","    # Load the image using OpenCV\n","    image = cv2.imread(image_path)\n","    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","\n","    # Get the bounding boxes and labels\n","    boxes = outputs[0]['boxes'].cpu().numpy()\n","    scores = outputs[0]['scores'].cpu().numpy()\n","\n","    # Draw the boxes on the image\n","    for i, box in enumerate(boxes):\n","        if scores[i] > threshold:\n","            # Convert box coordinates to integers\n","            box = box.astype(int)\n","            cv2.rectangle(image, (box[0], box[1]), (box[2], box[3]), color=(0, 255, 0), thickness=2)\n","\n","    # Save the image with detections\n","    result_image_path = os.path.join('results', os.path.basename(image_path))\n","    cv2.imwrite(result_image_path, cv2.cvtColor(image, cv2.COLOR_RGB2BGR))\n","\n","\n","def process_dataset(dataset_dir, model, transform):\n","    # Create a results directory if it doesn't exist\n","    if not os.path.exists('results'):\n","        os.makedirs('results')\n","\n","    # Iterate over all images in the directory\n","    for image_name in os.listdir(dataset_dir):\n","        image_path = os.path.join(dataset_dir, image_name)\n","\n","        if image_path.endswith(('.png', '.jpg', '.jpeg')):\n","            print(f\"Processing {image_name}...\")\n","            outputs = detect_objects(image_path, model, transform)\n","            visualize_results(image_path, outputs)\n","\n","def main():\n","    model = get_model()\n","    dataset_dir =  '/content/drive/MyDrive/Precog/Colab Notebooks/dataset'\n","    process_dataset(dataset_dir, model, get_transform())\n","\n","if __name__ == \"__main__\":\n","    main()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1200,"status":"ok","timestamp":1721408601541,"user":{"displayName":"Sriharitha","userId":"12204374650022354334"},"user_tz":-330},"id":"yGT__B_t7uyy","outputId":"74582770-65ca-4667-82c3-37520be73145"},"outputs":[{"name":"stdout","output_type":"stream","text":["Total objects detected: 0\n"]}],"source":["import os\n","import torch\n","import torchvision\n","from torchvision import transforms as T\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","import cv2\n","from collections import defaultdict\n","\n","\n","# COCO class labels\n","COCO_INSTANCE_CATEGORY_NAMES = [\n","    '__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n","    'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A', 'stop sign',\n","    'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant',\n","    'bear', 'zebra', 'giraffe', 'N/A', 'backpack', 'umbrella', 'N/A', 'N/A', 'handbag',\n","    'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat',\n","    'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle', 'N/A', 'wine glass',\n","    'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli',\n","    'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'N/A',\n","    'dining table', 'N/A', 'N/A', 'toilet', 'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard',\n","    'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N/A', 'book', 'clock',\n","    'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'\n","]\n","\n","\n","# Function to get the pretrained Faster R-CNN model\n","def get_model():\n","    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n","    model.eval()\n","    return model\n","\n","# Function to transform input image\n","def get_transform():\n","    transform = T.Compose([\n","        T.ToTensor(),\n","    ])\n","    return transform\n","# Function to perform inference on a single image\n","def detect_objects(image_path, model, transform):\n","    image = Image.open(image_path).convert(\"RGB\")\n","    image_tensor = transform(image).unsqueeze(0)\n","    with torch.no_grad():\n","        outputs = model(image_tensor)\n","    return outputs\n","\n","def visualize_and_catalog(image_path, outputs, object_catalog, threshold=0.5):\n","    image = cv2.imread(image_path)\n","    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","    boxes = outputs[0]['boxes'].cpu().numpy()\n","    scores = outputs[0]['scores'].cpu().numpy()\n","    labels = outputs[0]['labels'].cpu().numpy()\n","\n","    for i, box in enumerate(boxes):\n","        if scores[i] > threshold:\n","            # Convert box coordinates to integers\n","            box = box.astype(int)\n","            cv2.rectangle(image, (box[0], box[1]), (box[2], box[3]), color=(0, 255, 0), thickness=2)\n","            label = labels[i]\n","            object_catalog[COCO_INSTANCE_CATEGORY_NAMES[label]] += 1\n","\n","    result_image_path = os.path.join('results', os.path.basename(image_path))\n","    cv2.imwrite(result_image_path, cv2.cvtColor(image, cv2.COLOR_RGB2BGR))\n","\n","\n","\n","def process_dataset(dataset_dir, model, transform):\n","    if not os.path.exists('results'):\n","        os.makedirs('results')\n","\n","    object_catalog = defaultdict(int)\n","\n","    for image_name in os.listdir(dataset_dir):\n","        image_path = os.path.join(dataset_dir, image_name)\n","        if image_path.endswith(('.png', '.jpg', '.jpeg')):\n","            print(f\"Processing {image_name}...\")\n","            outputs = detect_objects(image_path, model, transform)\n","            visualize_and_catalog(image_path, outputs, object_catalog)\n","\n","    return object_catalog\n","\n","\n","def main():\n","    model = get_model()\n","    dataset_dir =  '/content/drive/MyDrive/Precog/Colab Notebooks/dataset'\n","    object_catalog = process_dataset(dataset_dir, model, get_transform())\n","\n","    total_objects = sum(object_catalog.values())\n","    print(f\"Total objects detected: {total_objects}\")\n","    for obj, count in object_catalog.items():\n","        print(f\"{obj}: {count} ({(count/total_objects)*100:.2f}%)\")\n","\n","if __name__ == \"__main__\":\n","    main()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TViGzDCaKbNm"},"outputs":[],"source":["'''import cv2\n","import os\n","\n","def load_images_from_folder(folder):\n","    images = []\n","    if not os.path.exists(folder):\n","        print(f\"Folder '{folder}' does not exist.\")\n","        return images\n","\n","    for filename in os.listdir(folder):\n","        file_path = os.path.join(folder, filename)\n","        if os.path.isfile(file_path):\n","            img = cv2.imread(file_path)\n","            if img is not None:\n","                images.append(img)\n","            else:\n","                print(f\"Failed to load image: {file_path}\")\n","        else:\n","            print(f\"Skipping non-file entry: {file_path}\")\n","\n","    if not images:\n","        print(\"No images were loaded.\")\n","    return images\n","\n","folder_path = '/content/drive/MyDrive/Precog/Colab Notebooks/dataset'\n","images = load_images_from_folder(folder_path)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1-B-3eucZHtS"},"outputs":[],"source":["import torch\n","from torchvision import models, transforms\n","\n","# Load a pre-trained object detection model (e.g., Faster R-CNN)\n","model = models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n","model.eval()\n","\n","# Define a transform to convert images to the format expected by the model\n","transform = transforms.Compose([\n","    transforms.ToPILImage(),\n","    transforms.ToTensor(),\n","])\n","\n","def detect_objects(image, model, transform):\n","    image_tensor = transform(image)\n","    image_tensor = image_tensor.unsqueeze(0)\n","    with torch.no_grad():\n","        outputs = model(image_tensor)\n","    return outputs[0]\n","\n","detected_objects = [detect_objects(img, model, transform) for img in images]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zF7-QuOrpd7A"},"outputs":[],"source":["import pandas as pd\n","\n","def extract_objects(detections, threshold=0.5):\n","    objects = []\n","    for detection in detections:\n","        for idx, score in enumerate(detection['scores']):\n","            if score > threshold:\n","                label = detection['labels'][idx].item()\n","                objects.append(label)\n","    return objects\n","\n","object_catalog = [extract_objects(d) for d in detected_objects]\n","flattened_objects = [item for sublist in object_catalog for item in sublist]\n","object_df = pd.DataFrame(flattened_objects, columns=['object'])\n","object_distribution = object_df['object'].value_counts()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ohOob-jEpkVi"},"outputs":[],"source":["# Assume you have a DataFrame with image paths and toxicity labels\n","# e.g., meme_data = pd.DataFrame({'image_path': ['path1', 'path2', ...], 'toxic': [1, 0, ...]})\n","\n","def assign_toxicity_score(objects, toxic_objects, non_toxic_objects):\n","    score = 0\n","    for obj in objects:\n","        if obj in toxic_objects:\n","            score += 1\n","        elif obj in non_toxic_objects:\n","            score -= 1\n","    return score\n","\n","# Get sets of objects from toxic and non-toxic memes\n","toxic_objects = set([obj for idx, obj_list in enumerate(object_catalog) if meme_data['toxic'][idx] == 1 for obj in obj_list])\n","non_toxic_objects = set([obj for idx, obj_list in enumerate(object_catalog) if meme_data['toxic'][idx] == 0 for obj in obj_list])\n","\n","# Calculate toxicity scores for all memes\n","meme_data['toxicity_score'] = meme_data.apply(lambda row: assign_toxicity_score(object_catalog[row.name], toxic_objects, non_toxic_objects), axis=1)\n","\n","# Simple classification: If score > 0, classify as toxic; else non-toxic\n","meme_data['predicted_toxic'] = meme_data['toxicity_score'] > 0\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BsCiz29uppZg"},"outputs":[],"source":["from sklearn.metrics import accuracy_score, classification_report\n","\n","accuracy = accuracy_score(meme_data['toxic'], meme_data['predicted_toxic'])\n","report = classification_report(meme_data['toxic'], meme_data['predicted_toxic'])\n","\n","print(f\"Accuracy: {accuracy}\")\n","print(\"Classification Report:\")\n","print(report)\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"authorship_tag":"ABX9TyMIykkzdZKhZSXsu6hZx2w+"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}